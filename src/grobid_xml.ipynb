{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML Parse Eden Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tek bir xml dosyasında test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pickle\n",
    "from Levenshtein import distance\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tei_doc = 'grobid_xmls/dergi1_24.grobid.tei.xml'\n",
    "with open(tei_doc, 'r') as tei:\n",
    "    soup = BeautifulSoup(tei, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.getText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerekli Fonksiyonlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_directory_quoted(directory_path):\n",
    "    files_dict = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    files_dict[filename[:-4]] = []\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading '{filename}': {e}\")\n",
    "                continue\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_directory_quoters(directory_path):\n",
    "    folder_dict = {}\n",
    "    for foldername in os.listdir(directory_path):\n",
    "        folder_path = os.path.join(directory_path, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Read files in the directory\n",
    "            files_dict = read_files_in_directory_quoted(folder_path)\n",
    "            # Check if the dictionary is not empty before adding it\n",
    "            if files_dict:\n",
    "                folder_dict[foldername] = files_dict\n",
    "    return folder_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_directory_quoters_alternative(directory_path):\n",
    "    folder_dict = {}\n",
    "    for foldername in os.listdir(directory_path):\n",
    "        folder_path = os.path.join(directory_path, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            folder_dict[foldername] = []\n",
    "    return folder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrounding_text(tag, context_size=30):\n",
    "    text = tag.parent.get_text()  # Get the text of the parent element\n",
    "    ref_text = tag.get_text()\n",
    "    ref_index = text.find(ref_text)\n",
    "\n",
    "    start_index = max(ref_index - context_size, 0)\n",
    "    end_index = ref_index + len(ref_text) + context_size\n",
    "\n",
    "    context = text[start_index:end_index]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract bibliographic information from a <biblStruct>\n",
    "def extract_bibl_info(bibl_struct):\n",
    "    title_tag = bibl_struct.find('title')\n",
    "    title = title_tag.get_text() if title_tag else ''\n",
    "    \n",
    "    author_tags = bibl_struct.find_all('author')\n",
    "    authors = []\n",
    "    for author in author_tags:\n",
    "        forename_tag = author.find('forename')\n",
    "        surname_tag = author.find('surname')\n",
    "        forename = forename_tag.get_text() if forename_tag else ''\n",
    "        surname = surname_tag.get_text() if surname_tag else ''\n",
    "        author_name = f\"{forename} {surname}\".strip()\n",
    "        if author_name:\n",
    "            authors.append(author_name)\n",
    "    \n",
    "    date_tag = bibl_struct.find('date')\n",
    "    date = date_tag.get_text() if date_tag else ''\n",
    "\n",
    "    #bibl_info = f\"{title} by {', '.join(authors)}. {date}.\"\n",
    "    bibl_info = (title, authors, date)\n",
    "    return bibl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ref_tags = soup.find_all('ref', {'type': 'bibr'})\n",
    "filtered_ref_tags_on_ref = soup.find_all('biblstruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ref_tags_on_ref_text = [e.getText() for e in filtered_ref_tags_on_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tek xml'i parse etmek için loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_and_bibl_info_single = []\n",
    "\n",
    "for ref in filtered_ref_tags:\n",
    "    # Get the surrounding text\n",
    "    context_text = get_surrounding_text(ref)\n",
    "\n",
    "    # Get the reference ID\n",
    "    ref_id = ref.get('target')\n",
    "    if ref_id:\n",
    "        ref_id = ref_id.lstrip('#')\n",
    "        print(ref_id)\n",
    "        # Find the corresponding <biblStruct>\n",
    "        bibl_struct = soup.find('biblstruct', {'xml:id': ref_id})\n",
    "\n",
    "        if bibl_struct:\n",
    "            bibl_info = extract_bibl_info(bibl_struct)\n",
    "            context_and_bibl_info_single.append((context_text, bibl_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, bibl_info in context_and_bibl_info_single:\n",
    "    print(\"Context:\")\n",
    "    print(context)\n",
    "    print(\"Bibliographic Information:\")\n",
    "    print(bibl_info)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birden Fazla XML için Parse Etmek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_and_bibl_info_all = []\n",
    "folder_path = 'grobid_xmls' \n",
    "for filename in os.listdir(folder_path):\n",
    "\n",
    "    context_and_bibl_info = []\n",
    "\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with open(file_path, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "    \n",
    "\n",
    "    filtered_ref_tags = soup.find_all('ref', {'type': 'bibr'})\n",
    "    \n",
    "\n",
    "    title_tag_xml = soup.find('title')\n",
    "    title_of_xml = title_tag_xml.get_text() if title_tag_xml else ''\n",
    "    \n",
    "    #title_of_xml = soup.title.getText()\n",
    "    context_and_bibl_info.append(title_of_xml)\n",
    "\n",
    "    for ref in filtered_ref_tags:\n",
    "    # Get the surrounding text\n",
    "        context_text = get_surrounding_text(ref, context_size=50)\n",
    "\n",
    "        # Get the reference ID\n",
    "        ref_id = ref.get('target')\n",
    "        if ref_id:\n",
    "            ref_id = ref_id.lstrip('#')\n",
    "            # Find the corresponding <biblStruct>\n",
    "            bibl_struct = soup.find('biblstruct', {'xml:id': ref_id})\n",
    "            if bibl_struct:\n",
    "                bibl_info = extract_bibl_info(bibl_struct)\n",
    "                context_and_bibl_info.append((context_text, bibl_info))\n",
    "    \n",
    "    context_and_bibl_info_all.append(context_and_bibl_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_and_bibl_info_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quoters Yapısına Uygun Şekilde Parse Etmek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_0 = read_files_in_directory_quoters_alternative('all_all')\n",
    "dict_0_clone = copy.deepcopy(dict_0)\n",
    "dict_0_final = copy.deepcopy(dict_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_0_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_and_bibl_info_all_0 = []\n",
    "folder_path_0 = 'all_all'\n",
    "\n",
    "# Start iterating over folder named as 0\n",
    "for folder in os.listdir(folder_path_0):\n",
    "    folder_path = os.path.join(folder_path_0, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        # Iterate over files inside of the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "\n",
    "            # file adı .DS_Store ise devam et\n",
    "            if file == '.DS_Store' or file.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            \n",
    "            context_and_bibl_info_0 = []\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            # tei.xml dosyasını aç ve BeautifulSoup objesine dönüştür\n",
    "            with open(file_path, 'r') as tei:\n",
    "                soup = BeautifulSoup(tei, 'lxml')\n",
    "            \n",
    "            # metin içi refaransları bul\n",
    "            filtered_ref_tags = soup.find_all('ref', {'type': 'bibr'})\n",
    "            \n",
    "            # metin içi refaransları dolaş\n",
    "            for ref in filtered_ref_tags:\n",
    "\n",
    "                context_text = get_surrounding_text(ref, context_size=50)\n",
    "                ref_id = ref.get('target')\n",
    "                if ref_id:\n",
    "                    ref_id = ref_id.lstrip('#')\n",
    "                    bibl_struct = soup.find('biblstruct', {'xml:id': ref_id})\n",
    "                    if bibl_struct:\n",
    "                        quoted_title_tag = bibl_struct.find('title')\n",
    "                        quoted_title = quoted_title_tag.get_text() if quoted_title_tag else ''\n",
    "                        \n",
    "                        if fuzz.ratio(quoted_title.lower(), folder.lower()) > 50:\n",
    "                            dict_0[folder].append(context_text)\n",
    "                            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractı yanına eklemek için kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2700 = '/Users/borankahraman/ITU/lcr/articles_2700.pickle'\n",
    "\n",
    "with open(articles_2700, 'rb') as file:\n",
    "    articles = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keys to keep\n",
    "keys_to_keep = ['ArticleTitle', 'Author', 'Date', 'Abstract']\n",
    "\n",
    "# Filter and transform the articles\n",
    "articles_filtered = [\n",
    "    (\n",
    "        article.get('ArticleTitle', [' '])[0],\n",
    "        article.get('Author', [' '])[0],\n",
    "        article.get('Abstract', [' '])[0],\n",
    "        article.get('Date', [' '])[0]\n",
    "    )\n",
    "    for article in articles\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_0_keys = list(dict_0.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_articles_filtered = [e for e in articles_filtered if e[0][0] in '0123456789']\n",
    "int_dict_0_keys = [e for e in dict_0_keys if e[0] in '0123456789']\n",
    "a_articles_filtered = [e for e in articles_filtered if e[0][0] == 'a' or e[0][0] == 'A']\n",
    "a_dict_0_keys = [e for e in dict_0_keys if e[0] == 'a' or e[0] == 'A']\n",
    "b_articles_filtered = [e for e in articles_filtered if e[0][0] == 'b' or e[0][0] == 'B']\n",
    "b_dict_0_keys = [e for e in dict_0_keys if e[0] == 'b' or  e[0] == 'B']\n",
    "c_articles_filtered = [e for e in articles_filtered if e[0][0] == 'c' or e[0][0] == 'C']\n",
    "c_dict_0_keys = [e for e in dict_0_keys if e[0] == 'c' or e[0] == 'C']\n",
    "ç_dict_0_keys = [e for e in dict_0_keys if e[0] == 'ç' or e[0] == 'Ç']\n",
    "ç_articles_filtered = [e for e in articles_filtered if e[0][0] == 'ç' or e[0][0] == 'Ç']\n",
    "d_articles_filtered = [e for e in articles_filtered if e[0][0] == 'd' or e[0][0] == 'D']\n",
    "d_dict_0_keys = [e for e in dict_0_keys if e[0] == 'd' or e[0] == 'D'] \n",
    "e_articles_filtered = [e for e in articles_filtered if e[0][0] == 'e' or e[0][0] == 'E']\n",
    "e_dict_0_keys = [e for e in dict_0_keys if e[0] == 'e' or e[0] == 'E']\n",
    "f_articles_filtered = [e for e in articles_filtered if e[0][0] == 'f' or e[0][0] == 'F']\n",
    "f_dict_0_keys = [e for e in dict_0_keys if e[0] == 'f' or e[0] == 'F' ]\n",
    "g_articles_filtered = [e for e in articles_filtered if e[0][0] == 'g' or e[0][0] == 'G']\n",
    "g_dict_0_keys = [e for e in dict_0_keys if e[0] == 'g' or e[0] == 'G']\n",
    "h_articles_filtered = [e for e in articles_filtered if e[0][0] == 'h' or e[0][0] == 'H']\n",
    "h_dict_0_keys = [e for e in dict_0_keys if e[0] == 'h' or e[0] == 'H']\n",
    "i_articles_filtered = [e for e in articles_filtered if e[0][0] == 'i' or e[0][0] == 'I']\n",
    "i_dict_0_keys = [e for e in dict_0_keys if e[0] == 'i' or e[0] == 'I']\n",
    "j_articles_filtered = [e for e in articles_filtered if e[0][0] == 'j' or e[0][0] == 'J']\n",
    "j_dict_0_keys = [e for e in dict_0_keys if e[0] == 'j' or e[0] == 'J']\n",
    "k_articles_filtered = [e for e in articles_filtered if e[0][0] == 'k' or e[0][0] == 'K']\n",
    "k_dict_0_keys = [e for e in dict_0_keys if e[0] == 'k' or e[0] == 'K']\n",
    "l_articles_filtered = [e for e in articles_filtered if e[0][0] == 'l' or e[0][0] == 'L']\n",
    "l_dict_0_keys = [e for e in dict_0_keys if e[0] == 'l' or e[0] == 'L']\n",
    "m_articles_filtered = [e for e in articles_filtered if e[0][0] == 'm' or e[0][0] == 'M']\n",
    "m_dict_0_keys = [e for e in dict_0_keys if e[0] == 'm' or e[0] == 'M']\n",
    "n_articles_filtered = [e for e in articles_filtered if e[0][0] == 'n' or e[0][0] == 'N']\n",
    "n_dict_0_keys = [e for e in dict_0_keys if e[0] == 'n' or e[0] == 'N']\n",
    "o_articles_filtered = [e for e in articles_filtered if e[0][0] == 'o' or e[0][0] == 'O']\n",
    "o_dict_0_keys = [e for e in dict_0_keys if e[0] == 'o' or e[0] == 'O']\n",
    "p_articles_filtered = [e for e in articles_filtered if e[0][0] == 'p' or e[0][0] == 'P']\n",
    "p_dict_0_keys = [e for e in dict_0_keys if e[0] == 'p' or e[0] == 'P']\n",
    "q_articles_filtered = [e for e in articles_filtered if e[0][0] == 'q' or e[0][0] == 'Q']\n",
    "q_dict_0_keys = [e for e in dict_0_keys if e[0] == 'q' or e[0] == 'Q']\n",
    "r_articles_filtered = [e for e in articles_filtered if e[0][0] == 'r' or e[0][0] == 'R']\n",
    "r_dict_0_keys = [e for e in dict_0_keys if e[0] == 'r' or e[0] == 'R']\n",
    "s_articles_filtered = [e for e in articles_filtered if e[0][0] == 's' or e[0][0] == 'S']\n",
    "s_dict_0_keys = [e for e in dict_0_keys if e[0] == 's' or e[0] == 'S']\n",
    "t_articles_filtered = [e for e in articles_filtered if e[0][0] == 't' or e[0][0] == 'T']\n",
    "t_dict_0_keys = [e for e in dict_0_keys if e[0] == 't' or e[0] == 'T'] \n",
    "u_articles_filtered = [e for e in articles_filtered if e[0][0] == 'u' or e[0][0] == 'U']\n",
    "u_dict_0_keys = [e for e in dict_0_keys if e[0] == 'u' or e[0] == 'U']\n",
    "v_articles_filtered = [e for e in articles_filtered if e[0][0] == 'v' or e[0][0] == 'V']\n",
    "v_dict_0_keys = [e for e in dict_0_keys if e[0] == 'v' or e[0] == 'V']\n",
    "w_articles_filtered = [e for e in articles_filtered if e[0][0] == 'w' or e[0][0] == 'W']\n",
    "w_dict_0_keys = [e for e in dict_0_keys if e[0] == 'w' or e[0] == 'W']\n",
    "x_articles_filtered = [e for e in articles_filtered if e[0][0] == 'x' or e[0][0] == 'X']\n",
    "x_dict_0_keys = [e for e in dict_0_keys if e[0] == 'x' or e[0] == 'X'] \n",
    "y_articles_filtered = [e for e in articles_filtered if e[0][0] == 'y' or e[0][0] == 'Y']\n",
    "y_dict_0_keys = [e for e in dict_0_keys if e[0] == 'y' or e[0][0] == 'Y']\n",
    "z_articles_filtered = [e for e in articles_filtered if e[0][0] == 'z' or e[0][0] == 'Z']   \n",
    "z_dict_0_keys = [e for e in dict_0_keys if e[0] == 'z' or e[0] == 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_strings_alt(list1, list2, in_dict):\n",
    "    min_size = min(len(list1), len(list2))\n",
    "    matched_pairs = []\n",
    "\n",
    "    for pair in itertools.product(list1, list2):\n",
    "        #print(pair)\n",
    "        str2_not_lowered = pair[1]\n",
    "        str1, str2 = pair[0][0].lower(), pair[1].lower()\n",
    "        dist = distance(str1, str2)\n",
    "        similarity = 1 - (dist / max(len(str1), len(str2)))\n",
    "        # pair[0][1] -> author, pair[0][2] -> abstract, pair[0][3] -> date\n",
    "        res = (similarity, str2_not_lowered, pair[0][1], pair[0][2], pair[0][3]) \n",
    "        matched_pairs.append(res)\n",
    "\n",
    "    matched_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    matched_pairs = matched_pairs[:min_size]\n",
    "    #print(matched_pairs)\n",
    "    for pair in matched_pairs:\n",
    "        in_dict[pair[1]].append(pair[2]) # author\n",
    "        in_dict[pair[1]].append(pair[3]) # abstract\n",
    "        in_dict[pair[1]].append(pair[4]) # date\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"on integers\")\n",
    "match_strings_alt(int_articles_filtered, int_dict_0_keys, dict_0_clone)\n",
    "print(\"on a\")\n",
    "match_strings_alt(a_articles_filtered, a_dict_0_keys, dict_0_clone)\n",
    "print(\"on b\")\n",
    "match_strings_alt(b_articles_filtered, b_dict_0_keys, dict_0_clone)\n",
    "print(\"on c\")\n",
    "match_strings_alt(c_articles_filtered, c_dict_0_keys, dict_0_clone)\n",
    "print(\"on d\")\n",
    "match_strings_alt(d_articles_filtered, d_dict_0_keys, dict_0_clone)\n",
    "print(\"on e\")\n",
    "match_strings_alt(e_articles_filtered, e_dict_0_keys, dict_0_clone)\n",
    "print(\"on f\")\n",
    "match_strings_alt(f_articles_filtered, f_dict_0_keys, dict_0_clone)\n",
    "print(\"on g\")\n",
    "match_strings_alt(g_articles_filtered, g_dict_0_keys, dict_0_clone)\n",
    "print(\"on h\")\n",
    "match_strings_alt(h_articles_filtered, h_dict_0_keys, dict_0_clone)\n",
    "print(\"on i\")\n",
    "match_strings_alt(i_articles_filtered, i_dict_0_keys, dict_0_clone)\n",
    "print(\"on j\")\n",
    "match_strings_alt(j_articles_filtered, j_dict_0_keys, dict_0_clone)\n",
    "print(\"on k\")\n",
    "match_strings_alt(k_articles_filtered, k_dict_0_keys, dict_0_clone)\n",
    "print(\"on l\")\n",
    "match_strings_alt(l_articles_filtered, l_dict_0_keys, dict_0_clone)\n",
    "print(\"on m\")\n",
    "match_strings_alt(m_articles_filtered, m_dict_0_keys, dict_0_clone)\n",
    "print(\"on n\")\n",
    "match_strings_alt(n_articles_filtered, n_dict_0_keys, dict_0_clone)\n",
    "print(\"on o\")\n",
    "match_strings_alt(o_articles_filtered, o_dict_0_keys, dict_0_clone)\n",
    "print(\"on p\")\n",
    "match_strings_alt(p_articles_filtered, p_dict_0_keys, dict_0_clone)\n",
    "print(\"on q\")\n",
    "match_strings_alt(q_articles_filtered, q_dict_0_keys, dict_0_clone)\n",
    "print(\"on r\")\n",
    "match_strings_alt(r_articles_filtered, r_dict_0_keys, dict_0_clone)\n",
    "print(\"on s\")\n",
    "match_strings_alt(s_articles_filtered, s_dict_0_keys, dict_0_clone)\n",
    "print(\"on t\")\n",
    "match_strings_alt(t_articles_filtered, t_dict_0_keys, dict_0_clone)\n",
    "print(\"on u\")\n",
    "match_strings_alt(u_articles_filtered, u_dict_0_keys, dict_0_clone)\n",
    "print(\"on v\")\n",
    "match_strings_alt(v_articles_filtered, v_dict_0_keys, dict_0_clone)\n",
    "print(\"on w\")\n",
    "match_strings_alt(w_articles_filtered, w_dict_0_keys, dict_0_clone)\n",
    "print(\"on x\")\n",
    "match_strings_alt(x_articles_filtered, x_dict_0_keys, dict_0_clone)\n",
    "print(\"on y\")\n",
    "match_strings_alt(y_articles_filtered, y_dict_0_keys, dict_0_clone)\n",
    "print(\"on z\")\n",
    "match_strings_alt(z_articles_filtered, z_dict_0_keys, dict_0_clone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_0_clone.keys():\n",
    "    if len(dict_0_clone[key]) != 0:\n",
    "        print(\"***********\")\n",
    "        print(key)\n",
    "        print(dict_0_clone[key])\n",
    "        print(len(dict_0_clone[key]))\n",
    "        print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['quoted_title', 'context', 'author', 'abstract', 'date']\n",
    "df = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_0_final.keys():\n",
    "    e_tuple = dict_0_final[key]\n",
    "    \n",
    "    first_e_tuple = e_tuple[0]\n",
    "    second_e_tuple = e_tuple[1]\n",
    "\n",
    "    if len(first_e_tuple) == 0:\n",
    "        continue\n",
    "\n",
    "    if len(second_e_tuple) > 1:\n",
    "        for e in second_e_tuple:\n",
    "            df = df._append({'quoted_title': key, 'quoter_context': e, 'quoted_author': first_e_tuple[0], 'quoted_abstract': first_e_tuple[1], 'quoted_date': first_e_tuple[2]}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('lcr_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
